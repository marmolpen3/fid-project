---
title: "R Notebook"
output: html_notebook
---

Instalación de ***tidyverse*** para la manipulación de los datos y la visualización, de ***Visadat*** para búsqueda de valores nulos y de ***ggplot2*** para visualización.

```{r}
library(tidyverse)
library(visdat)
library(ggplot2)
library(NbClust)
library(factoextra)
```

# Introducción

En este notebook, se pretende estudiar los patrones existentes dentro del dataset empleado en la regresión. El objetivo es construir grupos con los ejemplos más similares y analizar el comportamiento de estos. Para ello, se aplicarán los siguientes algoritmos k-means y clustering jerárquico.

```{r}
# Lectura del dataset
train_bikes <- read.table("data/train.csv", na.strings="", header=TRUE, sep=";", dec=".")
head(train_bikes)
```

## Preprocesado

El atributo id no aporta información relevante, lo eliminaremos y el atributo *count* no tiene un nombre representativo, lo sustituiremos por *num_bikes*.

```{r}
train_bikes <- select(train_bikes, -id)
names(train_bikes)[11] <- "num_bikes"
head(train_bikes)
```

Tal y como ya hemos visto en el notebook anterior, el dataset está limpio de valores NA pero se observan algunos outliers en la variable de *windspeed*. Por tanto, pasamos a la eliminación de dichos valores.

```{r}
# Outliers en windspeed por encima de 32 km/h
outlier_min <- min(boxplot.stats(train_bikes$windspeed)$out)

bikes_without_liers <- train_bikes$windspeed[train_bikes$windspeed < outlier_min]

# Comprobación
boxplot(bikes_without_liers, horizontal = TRUE)
boxplot.stats(bikes_without_liers)

# Filtrado de dataframe
train_bikes <- filter(train_bikes, train_bikes$windspeed < outlier_min)

str(train_bikes)
head(train_bikes)
```

Finalmente, filtramos aquellos ejemplos que tienen una humedad menor que 30, ya que suponen un riesgo para la salud. (Visitar para más información sobre la [humedad](https://es.wikipedia.org/wiki/Humedad_relativa)).

```{r}
# Filtrado de dataframe
train_bikes <- filter(train_bikes, train_bikes$humidity > 30)

# Comprobación
boxplot(train_bikes$humidity, horizontal = TRUE)
```

```{r}
# Resumen
summary(train_bikes)
```

## Clustering

El objetivo es someter a los datos a un estudio de clustering y obtener los ejemplos con las mísmas características. Filtramos únicamente por los ejemplos de 2011.

```{r}
train_bikes_short <- subset(train_bikes, train_bikes$year == 2011)
```

Se eliminan algunas variables como el year y la temp que está correlacionada con atemp.

```{r}
train_bikes_short <- select(train_bikes_short, -year, -temp)

head(train_bikes_short)
```

Nos aseguramos de que todas las variables con las que trabajamos son de clase numérica aplicando la función *lapply*.

```{r}
lapply(train_bikes_short, class)
```

Utilizamos la función *summary* para estudiar la distribución de los datos.

```{r}
summary(train_bikes_short)
```

Observamos que los datos no están distribuidos. Por lo tanto, aplicamos la función *scale* para que estén escalados y poder aplicar clustering.

```{r}
scaled_bikes <- scale(train_bikes_short)

summary(scaled_bikes)
```

#### Clustering No jerarquizado

Tras escalar los datos correctamente, damos paso a la aplicación del algoritmo particional **k-means** con el fin de identificar similitudes entre los ejemplos. El primer paso es [estimar el número](https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_nbclust) de centros que debemos definir, para ello, existen varios métodos como son *silhouette*, *wss* o *gap_stat*. Se omite la ejecución con el modelo *gap_stat* por ser costoso al no converger los datos, el resultado obtenido es que el número de clusters óptimo es K = 10.

```{r}

fviz_nbclust(scaled_bikes, FUN = hcut, method = "silhouette")
fviz_nbclust(scaled_bikes, FUN = hcut, method = "wss")
#fviz_nbclust(scaled_bikes, FUN = hcut, method = "gap_stat")

```

Siguiendo las recomendaciones de los métodos *silhouette* y *wss*, se aplica el algoritmo **k-means** para k = 4. Es decir, se van a obtener 4 agrupaciones de los datos según sus características.

```{r}
# Aseguramos la reproducibilidad, establecemos la semilla del generador de números aleatorios
seed_val <- 10
set.seed(seed_val)
# Número de clusters
k = 4
# Aplicamos k-means
bikes_clust <- kmeans(scaled_bikes, centers = k)
# Ejemplos por grupos
bikes_clust$size
```

La cantidad de ejemplos que pertenece a cada cluster no se diferencia mucho, salvo el grupo 2. Los analizamos con mayor profundidad a través de gráficas.

```{r}
# Añadimos la columna adicional con el número del cluster al que pertenece cada ejemplo
train_bikes_short['bikes_clust'] = bikes_clust$cluster

# Creamos una gráfica usando como ejes la hora y el número de bicicletas
plot_one <- ggplot(train_bikes_short, aes(x=hour, y=num_bikes, color=as.factor(bikes_clust))) + geom_point()
plot_one 
```

```{r}
# Creamos una gráfica usando como ejes la temporada y el número de bicicletas
plot_one <- ggplot(train_bikes_short, aes(x=season, y=num_bikes, color=as.factor(bikes_clust))) + geom_point()
plot_one 
```

```{r}
# Creamos una gráfica usando como ejes las vacaciones y el número de bicicletas
plot_one <- ggplot(train_bikes_short, aes(x=holiday, y=num_bikes, color=as.factor(bikes_clust))) + geom_point()
plot_one 
```

```{r}
# Creamos una gráfica usando como ejes la sensación de temperatura y el número de bicicletas
plot_one <- ggplot(train_bikes_short, aes(x=atemp, y=num_bikes, color=as.factor(bikes_clust))) + geom_point()
plot_one 
```

```{r}
# Creamos una gráfica usando como ejes el tiempo y el número de bicicletas
plot_one <- ggplot(train_bikes_short, aes(x=weather, y=num_bikes, color=as.factor(bikes_clust))) + geom_point()
plot_one 
```

```{r}
# Creamos una gráfica usando como ejes la humedad y el número de bicicletas
plot_one <- ggplot(train_bikes_short, aes(x=humidity, y=num_bikes, color=as.factor(bikes_clust))) + geom_point()
plot_one
```

```{r}

fviz_cluster(list(data = as.matrix(scaled_bikes), cluster = bikes_clust$cluster), labels = 5)
```

De estas gráficas podemos obtener algunas conclusiones:

-   Cluster 1: Destaca principalmente por ser ejemplos que sea cual sea la estación del año, suelen tener mal tiempo y mayor humedad.

-   Cluster 2: Se dan todas las condiciones para que se alquilen más bicicletas, buen tiempo, buena temperatura, humedad óptima y en primavera, verano u otoño.

-   Cluster 3: Su principal característica es que son datos que pertenecen a la estación de invierno y por tanto, el número de bicicletas que se alquilan son menos durante el día.

-   Cluster 4: Ejemplos que suelen encontrarse entre las 12pm y las 6am.

-   Las horas en las que se alquilan menos bicicletas son de 12pm a 6am y normalmente suele darse en primavera, verano y otoño, rara vez en invierno. Suelen tener buenas condiciones temporales.

-   Durante el día se alquilan menos bicicletas en invierno que en primavera, verano y otoño.

#### Clustering Jerárquico

Existen varios métodos para aplicar el [Clustering Jerárquico](https://rpubs.com/mjimcua/clustering-jerarquico-en-r), vamos a hacer uso de los métodos complete y single. Para calcular la distancia entre ejemplos se emplea la función [dist](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/dist) que por defecto usa el método *euclidean.*

#### Clustering Jerárquico: Complete

El dendograma nos permite vincular los grupos a los que pertenecen los ejemplos. Es decir, las agrupaciones forman un árbol donde cada rama que nace es un cluster.

```{r}

hier_clust_complete <- hclust(dist(scaled_bikes), method= 'complete')

plot(hier_clust_complete, cex = 0.6, hang = -1)

```

En este caso, observamos en la altura 6-8 cuatro agrupaciones de datos que son similares entre sí. Sin embargo, en la altura 4-6 podemos destacar diez grupos de datos.

```{r}

plot(hier_clust_complete, cex = 0.6, hang = -1)
rect.hclust(hier_clust_complete, k = 4, border = 2:4)

```

Los resultados obtenidos tienen similitud con los conseguidos por **K-means**.

```{r}
sample_clusters <- cutree(hier_clust_complete, k = 4)
fviz_cluster(list(data = as.matrix(scaled_bikes), cluster = sample_clusters), labels = 5)
```

#### Clustering Jerárquico: Single

Al igual que el anterior, el dendograma obtenido al aplicar el clustering jerárquico nos permite vincular los grupos a los que pertenecen los ejemplos.

```{r}

hier_clust_single <- hclust(dist(scaled_bikes), method= 'single')

plot(hier_clust_single, cex = 0.6, hang = -1)
```

```{r}
plot(hier_clust_single, cex = 0.6, hang = -1)
rect.hclust(hier_clust_single, k = 4, border = 2:4)
```

Los resultados obtenidos no se parecen al los anteriores modelos aplicados.

```{r}
sample_clusters <- cutree(hier_clust_single, k = 4)
fviz_cluster(list(data = as.matrix(scaled_bikes), cluster = sample_clusters), labels = 5)
```

Debido a la dificultad de los datos y complejidad de las agrupaciones, consideramos adecuado seleccionar otro dataset más apropiado para la aplicación de agrupaciones.

# **Unsupervised Learning on Country Data**

El conjunto de datos que se ha seleccionado contiene información sobre valores sociales, económicos y de salud que determinan la situación del país a analizar.
